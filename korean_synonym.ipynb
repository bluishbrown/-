{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3f745df9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word is not only one\n",
      "Which one?\n"
     ]
    }
   ],
   "source": [
    "#들어갈 글 넣어주세요\n",
    "writing = '여야는 그동안 추경안 협상을 놓고 진통을 겪어왔으나 6·1 지방선거를 사흘 앞두고 전격 합의를 도출했다. 추경안은 오후 9시 예결위 추경안심사소위와 예결위 전체회의 의결을 거쳐 이날 오후 7시30분부터 소집되는 본회의에서 처리된다.이날 자정에는 21대 국회 전반기 국회의장을 맡은 박병석 국회의장의 임기가 종료될 예정이어서 여야가 벼랑 끝 협상을 통해 극적으로 합의에 이른 것이다.다만 여야는 막판까지 최대 쟁점이었던 손실보상과 관련한 소급 적용 및 소득 역전 문제에 대해서는 추후 협의를 이어가기로 했다.국민의힘 권성동, 더불어민주당 박홍근 원내대표는 이날 오전 박병석 국회의장 주재로 국회 본관 의장실에서 회동을 한 뒤 각각 기자간담회를 열어 이같이 밝혔다.'\n",
    "#바꿀 단어 넣어주세요\n",
    "word ='협상'\n",
    "if word in writing:\n",
    "    replaced = writing.replace(word, '[MASK]')\n",
    "    count=replaced.count('[MASK]')\n",
    "    if (count!=1):\n",
    "        print('word is not only one\\nWhich one?')\n",
    "        index=0 #0부터 세주세요.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5403d152",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TFBertForMaskedLM\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2c6dd8bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertForMaskedLM: ['cls.predictions.decoder.bias', 'bert.embeddings.position_ids']\n",
      "- This IS expected if you are initializing TFBertForMaskedLM from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertForMaskedLM from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "model = TFBertForMaskedLM.from_pretrained('klue/bert-base', from_pt=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"klue/bert-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c19151ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import FillMaskPipeline\n",
    "pip = FillMaskPipeline(model=model, tokenizer=tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ce576034",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "처리  (Score_Bert) 0.6455333828926086\n",
      "마련  (Score_Bert) 0.07418838143348694\n",
      "편성  (Score_Bert) 0.06503162533044815\n",
      "제출  (Score_Bert) 0.02327374555170536\n",
      "협상  (Score_Bert) 0.0214377511292696\n",
      "교섭 (Score_Word2vec) 0.7716535925865173\n",
      "회담 (Score_Word2vec) 0.6924641132354736\n",
      "휴전 (Score_Word2vec) 0.6457741260528564\n",
      "협정 (Score_Word2vec) 0.6088683605194092\n",
      "협의 (Score_Word2vec) 0.5915411710739136\n",
      "타결 (Score_Word2vec) 0.5895344018936157\n",
      "타협 (Score_Word2vec) 0.5881052017211914\n",
      "밀약 (Score_Word2vec) 0.5799190998077393\n",
      "양국 (Score_Word2vec) 0.5601722002029419\n",
      "수교 (Score_Word2vec) 0.5525155663490295\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "candi=pip(replaced)[index]\n",
    "redundant = []\n",
    "\n",
    "for i in range(len(candi)):\n",
    "    redundant += candi[i]['token_str']\n",
    "    print(candi[i]['token_str'],\" (Score_Bert)\",candi[i]['score'])\n",
    "    \n",
    "model2 = gensim.models.Word2Vec.load('ko/ko.bin')\n",
    "candi2 = model2.wv.most_similar(word)\n",
    "for i in range(len(candi2)):\n",
    "    if(candi2[i][0] not in redundant):\n",
    "        print(candi2[i][0],\"(Score_Word2vec)\",candi2[i][1])\n",
    "    else:\n",
    "        print(candi2[i][0],\"(Score_Word2vec)\",candi2[i][1],\"maybe best choice\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
